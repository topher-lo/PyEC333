{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "colonial-breast",
   "metadata": {},
   "source": [
    "# Introduction to MLE (and MAP)\n",
    "---\n",
    "## Overview\n",
    "We will be looking at parameter estimation in this notebook. In particular, we will be:\n",
    "- Defining distributions\n",
    "- Plotting likelihood functions\n",
    "- Estimating parameters\n",
    "- Comparing numerical methods (that estimate parameters even in the absence of a closed-form solution)\n",
    "\n",
    "I thought this would be a good opportunity to introduce maximum a posteriori (MAP), which is a Bayesian method for parameter estimation. MAP incorporates a prior distribution over the possible values of the parameter to be estimated.\n",
    "\n",
    "**CAUTION:** this is NOT an introduction to Bayesian inference. My intention is to provide examples where **prior information** *might* help with parameter estimation. Note:\n",
    "- MAP is not representative of Bayesian inference. A MAP estimate is a point estimate equal to the mode of the posterior distribution. In general, Bayesians consider parameters and hypotheses as random variables. As random variables, you can (in most cases) meaningfully report statistics of the posterior distribution other than the mode (e.g. mean, standard deviation).\n",
    "- There are unsolved problems with regards to the intepretation of prior probabilities. These problems are anathema to frequentists and divide (philosophically inclined) Bayesians (see https://plato.stanford.edu/entries/epistemology-bayesian/#PotPro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "welsh-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import itertools\n",
    "\n",
    "import autograd.numpy as np # We will be using autograd's extended np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from autograd import jacobian\n",
    "from autograd import hessian\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(12,9)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spoken-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-paste",
   "metadata": {},
   "source": [
    "# 1. Distributions and likelihood functions\n",
    "\n",
    "## MLE:\n",
    "- We will be using `scipy`: a Python library for scientific computing. \n",
    "- `scipy` was introduced in the previous notebook on limited binary outcomes. `scipy` was used to implement Tobit from scratch.\n",
    "- Do not worry if you had difficulty understanding the Tobit implementation! \n",
    "- The examples in this notebook are a lot more straightforward than the Tobit example, which required us to specify a piece-wise likelihood function. We will revisit the Tobit implementation at the end of this notebook.\n",
    "\n",
    "## MAP:\n",
    "- We will be using `pymc3`: a high-level Python library for Bayesian inference via probabalistic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-royalty",
   "metadata": {},
   "source": [
    "## 1.1 Normal distribution\n",
    "- Recall that the general form of the Gaussian probability distribution function (PDF) is given by:\n",
    "\n",
    "$$ {\\displaystyle f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attractive-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Let's define the pdf of the Normal distribution from scratch\n",
    "\n",
    "def gaussian_pdf(mu: float,\n",
    "                 sigma: float,\n",
    "                 x: float) -> float:\n",
    "    \"\"\"Given mu, sigma, and a single observation x, \n",
    "    return the value of the Gaussian pdf.\n",
    "    \"\"\"\n",
    "    mu.astype(float)\n",
    "    sigma.astype(float)\n",
    "    x.astype(float)\n",
    "    sigma_2 = sigma**2\n",
    "    pi = scipy.pi\n",
    "    a1 = (sigma_2*2*pi)**-(1/2)\n",
    "    a2 = -1/(2*sigma_2)*(x-mu)**2\n",
    "    y = a1*np.exp(a2)\n",
    "    return y\n",
    "\n",
    "\n",
    "# 2. Let's define the likelihood and log-likelihood\n",
    "\n",
    "def gaussian_likelihood(params, pdf, X, ll=False, neg=False):\n",
    "    \"\"\"Given params (mu, sigma), a Gaussian pdf, and a \n",
    "    vector X of observations, return the value of the Gaussian \n",
    "    likelihood function.\n",
    "    \n",
    "    Note: if ll=True, returns the log-likelihood value.\n",
    "    Note: if neg=True, returns negative value (only effective if ll=True)\n",
    "    \"\"\"\n",
    "    def _pdf(mu, sigma, ll):\n",
    "        \"\"\"Converts the pdf function into a sequence of\n",
    "        functions, where the inner-most function takes a \n",
    "        single argument x.\n",
    "\n",
    "        This design pattern is called \"currying\".\n",
    "        \"\"\"\n",
    "        def _pdf_x(x):\n",
    "            val = pdf(mu, sigma, x)\n",
    "            if ll:\n",
    "                # If likelihood value not equal 0\n",
    "                if abs(val) > 0:\n",
    "                    val = np.log(pdf(mu, sigma, x))\n",
    "                else:\n",
    "                    val = 0\n",
    "            return val\n",
    "        return _pdf_x\n",
    "    \n",
    "    mu, sigma = params[0], params[1]\n",
    "    \n",
    "    # Map X to the pdf / log-pdf function\n",
    "    val_seq = np.array(list(map(_pdf(mu, sigma, ll), X)))   \n",
    "    \n",
    "    # If likelihood\n",
    "    if not(ll):\n",
    "        # Apply the PRODUCT operator\n",
    "        value = np.prod(val_seq)\n",
    "    # If log-likelihood\n",
    "    else:\n",
    "        # Apply SUMMATION\n",
    "        value = sum(val_seq)\n",
    "        if neg:\n",
    "            value = -value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-society",
   "metadata": {},
   "source": [
    "## 1.2 Bernoulli distribution\n",
    "- Recall that the general form of the Bernoulli probability mass function (PMF) is given by:\n",
    "\n",
    "$$ {\\displaystyle f(x;p)= p^{x}(1-p)^{1-x}} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fiscal-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Let's define the Bernoulli distribution\n",
    "\n",
    "def bernoulli_pmf(p: float, x: int) -> float:\n",
    "    \"\"\"Given probability of success p and a single observation x, \n",
    "    return the value of the Bernoulli pmf.\n",
    "    \"\"\"\n",
    "    y = p**x*(1-p)**(1-x)\n",
    "    return y\n",
    "    \n",
    "\n",
    "# 2. Let's define the likelihood and log-likelihood\n",
    "\n",
    "def bernoulli_likelihood(p, pmf, X, ll=False, neg=False):\n",
    "    \"\"\"Given p, Bernoulli pmf, and a \n",
    "    vector X of observations, return the value of the Gaussian \n",
    "    likelihood function.\n",
    "    \n",
    "    Note: if ll=True, returns the log-likelihood value.\n",
    "    Note: if neg=True, returns negative value (only effective if ll=True)\n",
    "    \"\"\"\n",
    "    def _pmf(p, ll):\n",
    "        \"\"\"Converts the pmf function into a sequence of\n",
    "        functions, where the inner-most function takes a \n",
    "        single argument x.\n",
    "\n",
    "        This design pattern is called \"currying\".\n",
    "        \"\"\"\n",
    "        def _pmf_x(x):\n",
    "            val = pmf(p, x)\n",
    "            if ll:\n",
    "                # If likelihood value not equal 0\n",
    "                if abs(val) > 0:\n",
    "                    val = np.log(pmf(p, x))\n",
    "                else:\n",
    "                    val = 0\n",
    "            return val\n",
    "        return _pmf_x\n",
    "\n",
    "    # Map X to the pdf / log-pdf function\n",
    "    val_seq = np.array(list(map(_pmf(p, ll), X)))   \n",
    "    \n",
    "    # If likelihood\n",
    "    if not(ll):\n",
    "        # Apply the PRODUCT operator\n",
    "        value = np.prod(val_seq)\n",
    "    # If log-likelihood\n",
    "    else:\n",
    "        # Apply SUMMATION\n",
    "        value = sum(val_seq)\n",
    "        if neg:\n",
    "            value = -value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-visit",
   "metadata": {},
   "source": [
    "# 2. Plotting likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-treasure",
   "metadata": {},
   "source": [
    "## 2.1 Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "celtic-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let create a sample of size N=50 that is drawn from a normal distribution \n",
    "# with mean = 0 and sigma = 1\n",
    "\n",
    "mu, sigma, N = 0, 1, 50\n",
    "X_normal = np.random.normal(mu, sigma, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sexual-failure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the gaussian likelihood on a 3D plane (likelihood value, mu, sigma)\n",
    "# by defining a range of possible mu and sigma values.\n",
    "# Use numpy's linspace function to create an array of decimal step values\n",
    "\n",
    "mu_range = np.linspace(-1, 1, num=100)\n",
    "sigma_range = np.linspace(0.05, 4.5, num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate likelihood values\n",
    "\n",
    "# 1. Compute the Cartesian product of mu_range and sigma_range\n",
    "mu_sigma_seq = list(itertools.product(mu_range, sigma_range))\n",
    "\n",
    "# 2. Apply gaussian_likelihood in a list comprehension over sequence from 1.\n",
    "# 2.1 Likelihood\n",
    "likelihood_val_seq = [gaussian_likelihood((mu, sigma),\n",
    "                                          gaussian_pdf,\n",
    "                                          X_normal,\n",
    "                                          ll=False)\n",
    "                      for mu, sigma in mu_sigma_seq]\n",
    "\n",
    "# 2.2. Log-likliehood\n",
    "log_likelihood_val_seq = [gaussian_likelihood((mu, sigma),\n",
    "                                              gaussian_pdf,\n",
    "                                              X_normal,\n",
    "                                              ll=True)\n",
    "                          for mu, sigma in mu_sigma_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-objective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gaussian likelihood function as a contour plot\n",
    "\n",
    "x, y = zip(*mu_sigma_seq)\n",
    "z = likelihood_val_seq\n",
    "\n",
    "# Make the plot\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_trisurf(x, y, z, cmap=plt.cm.viridis, linewidth=0.2)\n",
    "\n",
    "ax.set_xlabel('mu')\n",
    "ax.set_ylabel('sigma')\n",
    " \n",
    "# Add a color bar which maps values to colors.\n",
    "surf = ax.plot_trisurf(x, y, z, cmap=plt.cm.viridis, linewidth=0.2)\n",
    "fig.colorbar( surf, shrink=0.5, aspect=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Gaussian log-likelihood function as a contour plot\n",
    "\n",
    "x, y = zip(*list(itertools.product(mu_range, sigma_range)))\n",
    "z = log_likelihood_val_seq\n",
    "\n",
    "# Make the plot\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_trisurf(x, y, z, cmap=plt.cm.viridis, linewidth=0.2)\n",
    "\n",
    "ax.set_xlabel('mu')\n",
    "ax.set_ylabel('sigma')\n",
    " \n",
    "# Add a color bar which maps values to colors.\n",
    "surf = ax.plot_trisurf(x, y, z, cmap=plt.cm.viridis, linewidth=0.2)\n",
    "fig.colorbar( surf, shrink=0.5, aspect=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-yorkshire",
   "metadata": {},
   "source": [
    "## 2.2. Bernoulli distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let create two samples of size N=5 that is drawn from a Bernoulli distribution\n",
    "# with p_1 = 0 and p_2 = 1 \n",
    "\n",
    "N = 5\n",
    "X_bernoulli_1 = [0]*N\n",
    "X_bernoulli_2 = [1]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get sequence of probability of success values\n",
    "prob_vals = np.linspace(0, 1, 10) \n",
    "\n",
    "# 2. Apply bernoulli_likelihood in a list comprehension over sequence from 1.\n",
    "# 2.1 Likelihood\n",
    "likelihood_val_seq_1 = [bernoulli_likelihood(p,\n",
    "                                             bernoulli_pmf,\n",
    "                                             X_bernoulli_1)\n",
    "                        for p in prob_vals]\n",
    "\n",
    "\n",
    "likelihood_val_seq_2 = [bernoulli_likelihood(p,\n",
    "                                             bernoulli_pmf,\n",
    "                                             X_bernoulli_2)\n",
    "                        for p in prob_vals]\n",
    "\n",
    "\n",
    "# 2.2 log likelihood\n",
    "log_likelihood_val_seq_1 = [bernoulli_likelihood(p,\n",
    "                                                 bernoulli_pmf,\n",
    "                                                 X_bernoulli_1,\n",
    "                                                 ll=True)\n",
    "                            for p in prob_vals]\n",
    "\n",
    "log_likelihood_val_seq_2 = [bernoulli_likelihood(p,\n",
    "                                                 bernoulli_pmf,\n",
    "                                                 X_bernoulli_2,\n",
    "                                                 ll=True)\n",
    "                            for p in prob_vals]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Bernoulli likelihoods\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "sc1 = sns.scatterplot(ax=axes[0], x=prob_vals, y=likelihood_val_seq_1)\n",
    "sc2 = sns.scatterplot(ax=axes[1], x=prob_vals, y=likelihood_val_seq_2)\n",
    "\n",
    "axes[0].set(xlabel='Probability of success', ylabel='Likelihood value', title='prob = 0')\n",
    "axes[1].set(xlabel='Probability of success', ylabel='Likelihood value', title='prob = 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Bernoulli log-likelihoods\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "sc1 = sns.scatterplot(ax=axes[0], x=prob_vals, y=log_likelihood_val_seq_1)\n",
    "sc2 = sns.scatterplot(ax=axes[1], x=prob_vals, y=log_likelihood_val_seq_2)\n",
    "\n",
    "# You can't take log(0), hence two 0 \"maximum\" values at the end points\n",
    "# In the bernoulli_likelihood function, I set log-likelihood to 0 if the likelihood is 0\n",
    "# Pls fix?\n",
    "\n",
    "axes[0].set(xlabel='Probability of success', ylabel='Log-likelihood value', title='prob = 0')\n",
    "axes[1].set(xlabel='Probability of success', ylabel='Log-likelihood value', title='prob = 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-hands",
   "metadata": {},
   "source": [
    "# 3. Numerical optimisation in Python\n",
    "- `scipy` only has a minimiser.\n",
    "- Documentation for `scipy`: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "- Therefore, we need to use negative likelihoods / negative log-likelihoods. This logic was implemented in the likelihood Python functions.\n",
    "- Let's perform MLE using numerical methods on the normally distributed random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-responsibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let create a sample of size N=50 that is drawn from a normal distribution \n",
    "# with mean = 0 and sigma = 1\n",
    "\n",
    "mu, sigma, N = 0, 1, 50\n",
    "X_normal = np.random.normal(mu, sigma, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our sensitivity plan\n",
    "# We will build a sensitivity plan grid to test\n",
    "# every combination of hyperparameters.\n",
    "\n",
    "SENSITIVITY_PLAN = {\n",
    "    'x0': [(-5, 10), (-5, 500)], # starting values (mu, sigma)\n",
    "    'method': ['Nelder-Mead',\n",
    "               'BFGS',\n",
    "               'CG'] # methods\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use nested for loops to search through the two hyperparameters \n",
    "# But can you think of a faster, more concise, and scalable (to multiple hyperparameters)\n",
    "# implementation using the map-reduce approach?\n",
    "\n",
    "hyperparams = []\n",
    "results = []\n",
    "for x0 in SENSITIVITY_PLAN['x0']:\n",
    "    for method in SENSITIVITY_PLAN['method']:\n",
    "        result = scipy.optimize.minimize(fun=gaussian_likelihood,\n",
    "                                         x0=np.asarray(x0), # Starting values as ndarray\n",
    "                                         args=(gaussian_pdf,\n",
    "                                               X_normal,\n",
    "                                               True,\n",
    "                                               True), # Fixed parameters to specify the negative log-likelihood function\n",
    "                                         method=method)\n",
    "        hyperparams.append(f'{x0} and {method}')\n",
    "        results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that scipy.optimize.minimize returns a Dictionary with optimization results \n",
    "# and meta-information\n",
    "\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all results\n",
    "\n",
    "sensitivity_test = pd.DataFrame({\n",
    "    'hyperparams': hyperparams,\n",
    "    'mu': [r.x[0] for r in results],\n",
    "    'sigma': [r.x[1] for r in results],\n",
    "}).set_index('hyperparams')\n",
    "sensitivity_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italic-bishop",
   "metadata": {},
   "source": [
    "### Note:\n",
    "- To approximate the standard errors, we will be using the Autograd library.\n",
    "- The code below follows https://rlhick.people.wm.edu/posts/mle-autograd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let approximate the standard errors\n",
    "\n",
    "args = (gaussian_pdf, X_normal, True, True)\n",
    "jacobian_ = jacobian(gaussian_likelihood) # Define the Jacobian using the autograd library\n",
    "hessian_ = hessian(gaussian_likelihood) # Define the Hessian using the autograd library\n",
    "\n",
    "result = scipy.optimize.minimize(fun=gaussian_likelihood,\n",
    "                                 x0=np.asarray([-5, 10]), # Starting values as ndarray\n",
    "                                 args=args, # Fixed parameters to specify the negative log-likelihood function\n",
    "                                 method='BFGS',\n",
    "                                 jac=jacobian_)\n",
    "\n",
    "information = np.transpose(hessian_(result.x, *args)) # Information matrix\n",
    "se = np.sqrt(np.diagonal(np.linalg.inv(information))) # Square root of the diagonal of the inverse Hessian\n",
    "print('mu SE = {}\\nsigma SE = {}'.format(se[0], se[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-diabetes",
   "metadata": {},
   "source": [
    "# 4. Tobit revisited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "SENSITIVITY_PLAN = {\n",
    "    'method': ['Nelder-Mead',\n",
    "               'BFGS',\n",
    "               'CG'] # methods\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = []\n",
    "results = []\n",
    "for method in SENSITIVITY_PLAN['method']:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-stomach",
   "metadata": {},
   "source": [
    "# 5. (Extra) MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-chest",
   "metadata": {},
   "source": [
    "## 5.1 Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-avatar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-carbon",
   "metadata": {},
   "source": [
    "## 5.2 Bernoulli distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-legislature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ec333] *",
   "language": "python",
   "name": "conda-env-ec333-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
